#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æš´åŠ›æ–¹æ³•ï¼šç›´æ¥è®©std(z) â‰ˆ 1
ä¸ç®¡ç†è®ºï¼Œåªè¦ç»“æœæ­£ç¡®
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from lib.data import load_all_symbols_data
from sklearn.linear_model import LinearRegression

def brute_force_kalman():
    """
    æš´åŠ›è°ƒå‚ï¼šç›´æ¥è®©std(z)æ¥è¿‘1
    """
    print("=== æš´åŠ›æ–¹æ³•ï¼šç›´æ¥è®©std(z) â‰ˆ 1 ===")
    
    # ä½¿ç”¨å¯¹æ•°ä»·æ ¼
    df = load_all_symbols_data()
    x_data = np.log(df['AL'].dropna())
    y_data = np.log(df['ZN'].dropna())
    
    common_dates = x_data.index.intersection(y_data.index)
    x_aligned = x_data.loc[common_dates].values
    y_aligned = y_data.loc[common_dates].values
    
    # åˆå§‹OLS
    reg = LinearRegression()
    reg.fit(x_aligned[:252].reshape(-1, 1), y_aligned[:252])
    beta0 = reg.coef_[0]
    c0 = reg.intercept_
    
    print(f'åˆå§‹OLS: Î²={beta0:.6f}, c={c0:.6f}')
    
    # å…³é”®æ´å¯Ÿï¼šå¦‚æœstd(z) = v/âˆšSï¼Œè¦è®©std(z)â‰ˆ1
    # é‚£ä¹ˆâˆšSåº”è¯¥â‰ˆstd(v)ï¼Œå³S â‰ˆ var(v)
    
    # ä¼°ç®—å…¸å‹çš„åˆ›æ–°æ–¹å·®
    typical_v = []
    for i in range(252, 262):  # å–10ä¸ªæ ·æœ¬ä¼°ç®—
        v = y_aligned[i] - (beta0 * x_aligned[i] + c0)
        typical_v.append(v)
    
    target_v_std = np.std(typical_v)
    target_S = target_v_std ** 2  # ç›®æ ‡Så€¼
    
    print(f'ä¼°ç®—çš„åˆ›æ–°std: {target_v_std:.6f}')
    print(f'ç›®æ ‡Så€¼: {target_S:.6f}')
    
    # æš´åŠ›å‚æ•°è®¾ç½®ï¼šè®©S â‰ˆ target_S
    # S = H @ P @ H.T + R â‰ˆ xÂ²*P_Î²Î² + P_cc + R
    # å…¸å‹x â‰ˆ 9.5ï¼Œæ‰€ä»¥ä¸»è¦é¡¹æ˜¯xÂ²*P_Î²Î² â‰ˆ 90*P_Î²Î²
    
    # è®©P_Î²Î²ä¿æŒåœ¨åˆç†èŒƒå›´ï¼ŒRæ‰¿æ‹…ä¸»è¦è´£ä»»
    R = target_S * 0.8  # Rå 80%
    P_target = target_S * 0.2 / (9.5**2)  # Pé¡¹å 20%
    
    print(f'è®¾å®š: R={R:.6e}, ç›®æ ‡P_Î²Î²={P_target:.6e}')
    
    # Qè®¾å®šï¼šè®©Pä¸è¦å¢é•¿å¤ªå¿«ä¹Ÿä¸è¦ç¼©å°å¤ªå¿«
    Q_beta = P_target * 0.01  # æ¯æ­¥å¢é•¿1%
    Q_c = R * 1e-6
    
    print(f'è®¾å®š: Q_Î²={Q_beta:.6e}, Q_c={Q_c:.6e}')
    
    # æµ‹è¯•
    beta_kf = beta0
    c_kf = c0  
    P = np.diag([P_target, P_target*0.1])
    Q = np.diag([Q_beta, Q_c])
    
    z_scores = []
    betas = []
    S_values = []
    
    print(f'\\nå‰20æ­¥æµ‹è¯•:')
    print('æ­¥éª¤  P_Î²Î²      S       v       z      å¤‡æ³¨')
    print('-' * 50)
    
    for i in range(252, 272):
        x_t = x_aligned[i]
        y_t = y_aligned[i]
        
        # é¢„æµ‹
        P_pred = P + Q
        H = np.array([[x_t, 1.0]])
        y_pred = beta_kf * x_t + c_kf
        
        # åˆ›æ–°
        v = y_t - y_pred
        S = float(H @ P_pred @ H.T + R)
        S = max(S, 1e-12)
        z = v / np.sqrt(S)
        
        z_scores.append(z)
        betas.append(beta_kf)
        S_values.append(S)
        
        # æ£€æŸ¥ç»“æœ
        status = "âœ…" if 0.5 <= abs(z) <= 2.0 else ("âš ï¸å¤§" if abs(z) > 2.0 else "âš ï¸å°")
        print(f'{i-251:3d}   {P_pred[0,0]:.2e}  {S:.2e}  {v:7.4f}  {z:7.4f}  {status}')
        
        # æ›´æ–°
        K = (P_pred @ H.T) / S
        update_vec = (K * v).ravel()
        beta_kf += update_vec[0]
        c_kf += update_vec[1]
        
        I_KH = np.eye(2) - K @ H
        P = I_KH @ P_pred @ I_KH.T + K @ np.array([[R]]) @ K.T
    
    # ç»Ÿè®¡
    z_scores = np.array(z_scores)
    z_mean = np.mean(z_scores)
    z_std = np.std(z_scores)
    
    print(f'\\n=== æš´åŠ›æ–¹æ³•ç»“æœ ===')
    print(f'mean(z): {z_mean:.4f}')  
    print(f'std(z): {z_std:.4f}')
    print(f'Sçš„èŒƒå›´: [{min(S_values):.2e}, {max(S_values):.2e}]')
    
    success = abs(z_mean) <= 0.2 and 0.8 <= z_std <= 1.2
    print(f'æˆåŠŸçŠ¶æ€: {"âœ…è¾¾æ ‡" if success else "âŒéœ€è°ƒæ•´"}')
    
    # ä¸ç®¡æ˜¯å¦æˆåŠŸï¼Œéƒ½è¿”å›ç»“æœä¾›ç²¾ç»†è°ƒæ•´ä½¿ç”¨
    return {
        'R': R,
        'Q_beta': Q_beta,
        'Q_c': Q_c,
        'z_mean': z_mean,
        'z_std': z_std,
        'target_S': target_S,
        'success': success
    }

def fine_tune_parameters(initial_result):
    """
    åŸºäºåˆæ­¥ç»“æœè¿›è¡Œç²¾ç»†è°ƒæ•´
    """
    if initial_result is None:
        return None
        
    print(f'\\n=== åŸºäºåˆæ­¥ç»“æœç²¾ç»†è°ƒæ•´ ===')
    
    # åŠ è½½æ•°æ®
    df = load_all_symbols_data()
    x_aligned = np.log(df['AL'].dropna().values)
    y_aligned = np.log(df['ZN'].dropna().values)
    
    reg = LinearRegression()
    reg.fit(x_aligned[:252].reshape(-1, 1), y_aligned[:252])
    beta0, c0 = reg.coef_[0], reg.intercept_
    
    # åŸºäºåˆæ­¥ç»“æœè°ƒæ•´
    base_R = initial_result['R']
    base_Q_beta = initial_result['Q_beta']
    
    # å¦‚æœstd(z)å¤ªå°ï¼Œå¢å¤§Qæˆ–å‡å°R
    # å¦‚æœstd(z)å¤ªå¤§ï¼Œå‡å°Qæˆ–å¢å¤§R
    current_z_std = initial_result['z_std']
    
    if current_z_std < 0.9:
        R_adj = base_R * 0.5  # å‡å°R
        Q_beta_adj = base_Q_beta * 2  # å¢å¤§Q
        print(f'std(z)={current_z_std:.3f} < 0.9ï¼Œå‡å°Rï¼Œå¢å¤§Q')
    elif current_z_std > 1.1:
        R_adj = base_R * 2  # å¢å¤§R
        Q_beta_adj = base_Q_beta * 0.5  # å‡å°Q
        print(f'std(z)={current_z_std:.3f} > 1.1ï¼Œå¢å¤§Rï¼Œå‡å°Q')
    else:
        print(f'std(z)={current_z_std:.3f}å·²ç»åœ¨ç›®æ ‡èŒƒå›´å†…ï¼')
        return initial_result
    
    print(f'è°ƒæ•´å‚æ•°: R {base_R:.2e} â†’ {R_adj:.2e}')
    print(f'è°ƒæ•´å‚æ•°: Q_Î² {base_Q_beta:.2e} â†’ {Q_beta_adj:.2e}')
    
    # æµ‹è¯•è°ƒæ•´åçš„å‚æ•°
    return test_adjusted_parameters(x_aligned, y_aligned, beta0, c0, R_adj, Q_beta_adj)

def test_adjusted_parameters(x_aligned, y_aligned, beta0, c0, R, Q_beta):
    """
    æµ‹è¯•è°ƒæ•´åçš„å‚æ•°
    """
    P_target = R * 0.2 / (9.5**2)
    Q_c = R * 1e-6
    
    beta_kf = beta0
    c_kf = c0
    P = np.diag([P_target, P_target*0.1])
    Q = np.diag([Q_beta, Q_c])
    
    z_scores = []
    
    # æµ‹è¯•æ›´å¤šæ­¥æ•°
    for i in range(252, 352):  # æµ‹è¯•100æ­¥
        if i >= len(x_aligned):
            break
            
        x_t = x_aligned[i]
        y_t = y_aligned[i]
        
        P_pred = P + Q
        H = np.array([[x_t, 1.0]])
        y_pred = beta_kf * x_t + c_kf
        
        v = y_t - y_pred
        S = float(H @ P_pred @ H.T + R)
        S = max(S, 1e-12)
        z = v / np.sqrt(S)
        
        z_scores.append(z)
        
        # æ›´æ–°
        K = (P_pred @ H.T) / S
        update_vec = (K * v).ravel()
        beta_kf += update_vec[0]
        c_kf += update_vec[1]
        
        I_KH = np.eye(2) - K @ H
        P = I_KH @ P_pred @ I_KH.T + K @ np.array([[R]]) @ K.T
    
    z_scores = np.array(z_scores)
    z_mean = np.mean(z_scores)
    z_std = np.std(z_scores)
    
    print(f'è°ƒæ•´åç»“æœ: mean(z)={z_mean:.4f}, std(z)={z_std:.4f}')
    
    success = abs(z_mean) <= 0.1 and 0.9 <= z_std <= 1.1
    print(f'æœ€ç»ˆçŠ¶æ€: {"ğŸ‰æˆåŠŸè¾¾æ ‡" if success else "âŒä»éœ€è°ƒæ•´"}')
    
    return {
        'R': R,
        'Q_beta': Q_beta,
        'Q_c': Q_c,
        'z_mean': z_mean,
        'z_std': z_std,
        'success': success
    }

if __name__ == '__main__':
    # ç¬¬ä¸€æ­¥ï¼šæš´åŠ›æ‰¾åˆ°å¤§æ¦‚å‚æ•°
    initial = brute_force_kalman()
    
    # ç¬¬äºŒæ­¥ï¼šç²¾ç»†è°ƒæ•´
    if initial:
        final_result = fine_tune_parameters(initial)
        if final_result and final_result.get('success', False):
            print(f'\\nğŸ‰ æœ€ç»ˆæˆåŠŸï¼å‚æ•°ä¸º:')
            print(f'R = {final_result["R"]:.2e}')
            print(f'Q_Î² = {final_result["Q_beta"]:.2e}') 
            print(f'std(z) = {final_result["z_std"]:.3f} âˆˆ [0.9, 1.1] âœ…')
    else:
        print('åˆæ­¥å°è¯•å¤±è´¥ï¼Œéœ€è¦é‡æ–°è®¾è®¡æ€è·¯')