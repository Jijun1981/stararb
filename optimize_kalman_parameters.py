#!/usr/bin/env python3
"""
Kalmanæ»¤æ³¢å™¨å‚æ•°ä¼˜åŒ–çŸ©é˜µ
ç›®æ ‡ï¼šZ>2ä¿¡å·æ¯”ä¾‹2-5%ï¼ŒIRæœ€å¤§åŒ–ï¼Œä¸OLSç›¸å…³æ€§>0.6ï¼Œå¹³ç¨³æ€§ä¼˜è‰¯
"""
import pandas as pd
import numpy as np
import sys
import os
from itertools import product
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

sys.path.append('.')
from lib.data import load_all_symbols_data
from lib.coint import CointegrationAnalyzer
from lib.signal_generation import AdaptiveKalmanFilter
from statsmodels.tsa.stattools import adfuller
from sklearn.linear_model import LinearRegression
from scipy.stats import pearsonr

class KalmanParameterOptimizer:
    """Kalmanæ»¤æ³¢å™¨å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        # å‚æ•°æœç´¢ç©ºé—´
        self.delta_range = [0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96]
        self.lambda_range = [0.85, 0.87, 0.89, 0.90, 0.91, 0.92, 0.94]
        
        # ç›®æ ‡çº¦æŸï¼ˆæ”¾å®½çº¦æŸä»¥æ‰¾åˆ°å¯è¡Œè§£ï¼‰
        self.target_z_ratio = (0.01, 0.08)  # Z>2ä¿¡å·æ¯”ä¾‹1%-8%ï¼ˆæ”¾å®½ï¼‰
        self.min_ols_corr = 0.4              # ä¸OLSç›¸å…³æ€§>0.4ï¼ˆæ”¾å®½ï¼‰
        self.min_adf_pval = 0.10             # å¹³ç¨³æ€§è¦æ±‚ï¼ˆæ”¾å®½ï¼‰
        
        # æµ‹è¯•é…å¯¹ï¼ˆé€‰æ‹©ä»£è¡¨æ€§é…å¯¹ï¼‰
        self.test_pairs = [
            'AU-ZN',   # é«˜æ³¢åŠ¨é—®é¢˜é…å¯¹
            'CU-SN',   # ä¼˜ç§€é…å¯¹
            'ZN-SM',   # ä¸­ç­‰é…å¯¹  
            'RB-SM',   # ä¸­ç­‰é…å¯¹
            'SS-NI'    # ä¸­ç­‰é…å¯¹
        ]
        
    def optimize_parameters(self):
        """æ‰§è¡Œå‚æ•°ä¼˜åŒ–"""
        
        print("ğŸ”§ Kalmanæ»¤æ³¢å™¨å‚æ•°ä¼˜åŒ–çŸ©é˜µ")
        print("=" * 80)
        print(f"å‚æ•°ç©ºé—´: Î´{len(self.delta_range)} Ã— Î»{len(self.lambda_range)} = {len(self.delta_range) * len(self.lambda_range)}ç»„åˆ")
        print(f"æµ‹è¯•é…å¯¹: {len(self.test_pairs)}ä¸ª")
        print(f"ç›®æ ‡çº¦æŸ: Z>2æ¯”ä¾‹{self.target_z_ratio[0]*100:.0f}-{self.target_z_ratio[1]*100:.0f}%, OLSç›¸å…³>{self.min_ols_corr}, å¹³ç¨³p<{self.min_adf_pval}")
        
        # åŠ è½½æ•°æ®
        print("\nğŸ“Š åŠ è½½æ•°æ®...")
        data = load_all_symbols_data()
        
        # ç”Ÿæˆåæ•´é…å¯¹ä¿¡æ¯
        analyzer = CointegrationAnalyzer(data)
        coint_results = analyzer.screen_all_pairs(
            screening_windows=['1y'], 
            p_thresholds={'1y': 0.10},
            filter_logic='AND'
        )
        
        # è·å–æµ‹è¯•é…å¯¹çš„åŸºç¡€ä¿¡æ¯
        pair_info = {}
        for pair in self.test_pairs:
            if pair in coint_results['pair'].values:
                pair_data = coint_results[coint_results['pair'] == pair].iloc[0]
                pair_info[pair] = {
                    'symbol_x': pair.split('-')[0],
                    'symbol_y': pair.split('-')[1], 
                    'initial_beta': pair_data['beta_1y']
                }
            else:
                # æ‰‹åŠ¨è®¾ç½®
                symbols = pair.split('-')
                pair_info[pair] = {
                    'symbol_x': symbols[0],
                    'symbol_y': symbols[1],
                    'initial_beta': 1.0
                }
        
        print(f"é…å¯¹ä¿¡æ¯å‡†å¤‡å®Œæˆ: {len(pair_info)}ä¸ªé…å¯¹")
        
        # å‚æ•°ä¼˜åŒ–çŸ©é˜µ
        print("\nğŸš€ å¼€å§‹å‚æ•°ä¼˜åŒ–...")
        optimization_results = []
        total_combinations = len(self.delta_range) * len(self.lambda_range)
        
        for i, (delta, lambda_r) in enumerate(product(self.delta_range, self.lambda_range)):
            if i % 10 == 0:  # æ¯10ä¸ªç»„åˆæ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                print(f"\\nè¿›åº¦: {i+1}/{total_combinations} - Î´={delta:.2f}, Î»={lambda_r:.2f}")
            else:
                print(f"\\rè¿›åº¦: {i+1}/{total_combinations} - Î´={delta:.2f}, Î»={lambda_r:.2f}", end='')
            
            # æµ‹è¯•å½“å‰å‚æ•°ç»„åˆ
            combo_results = self._test_parameter_combination(delta, lambda_r, data, pair_info)
            
            if combo_results:
                optimization_results.append({
                    'delta': delta,
                    'lambda_r': lambda_r,
                    **combo_results
                })
        
        print("\\n\\nâœ… å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        
        # åˆ†æç»“æœ
        results_df = pd.DataFrame(optimization_results)
        if len(results_df) > 0:
            return self._analyze_optimization_results(results_df)
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„å‚æ•°ç»„åˆ")
            return None
    
    def _test_parameter_combination(self, delta, lambda_r, data, pair_info):
        """æµ‹è¯•å•ä¸ªå‚æ•°ç»„åˆ"""
        
        pair_results = []
        
        for pair, info in pair_info.items():
            try:
                result = self._evaluate_pair_performance(pair, info, delta, lambda_r, data)
                if result:
                    pair_results.append(result)
            except Exception as e:
                continue
        
        if len(pair_results) < 2:  # è‡³å°‘è¦æœ‰2ä¸ªé…å¯¹çš„ç»“æœï¼ˆé™ä½è¦æ±‚ï¼‰
            return None
        
        # æ±‡æ€»ç»“æœ
        avg_z_ratio = np.mean([r['z_gt2_ratio'] for r in pair_results])
        avg_ir = np.mean([r['ir'] for r in pair_results])
        avg_ols_corr = np.mean([r['ols_correlation'] for r in pair_results])
        avg_adf_pval = np.mean([r['adf_pvalue'] for r in pair_results])
        stationary_ratio = np.mean([r['is_stationary'] for r in pair_results])
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        score = self._calculate_composite_score(avg_z_ratio, avg_ir, avg_ols_corr, 
                                              avg_adf_pval, stationary_ratio)
        
        return {
            'avg_z_ratio': avg_z_ratio,
            'avg_ir': avg_ir,
            'avg_ols_corr': avg_ols_corr,
            'avg_adf_pval': avg_adf_pval,
            'stationary_ratio': stationary_ratio,
            'score': score,
            'valid_pairs': len(pair_results),
            'pair_details': pair_results
        }
    
    def _evaluate_pair_performance(self, pair, info, delta, lambda_r, data):
        """è¯„ä¼°å•ä¸ªé…å¯¹çš„è¡¨ç°"""
        
        symbol_x = info['symbol_x']
        symbol_y = info['symbol_y']
        
        # è·å–ä»·æ ¼æ•°æ®ï¼ˆä¿¡å·æœŸï¼‰
        signal_start_date = '2024-07-01'
        signal_end_date = '2025-08-20'
        
        # åŒ…å«90å¤©é¢„çƒ­æœŸ
        data_start_date = '2024-02-08'
        analysis_data = data[data_start_date:signal_end_date]
        
        if symbol_x not in analysis_data.columns or symbol_y not in analysis_data.columns:
            return None
        
        # ä»·æ ¼å¯¹é½
        x_prices = analysis_data[symbol_x].dropna()
        y_prices = analysis_data[symbol_y].dropna()
        common_dates = x_prices.index.intersection(y_prices.index)
        
        if len(common_dates) < 150:
            return None
        
        x_data = x_prices[common_dates].values
        y_data = y_prices[common_dates].values
        dates = common_dates
        
        # åˆ›å»ºKalmanæ»¤æ³¢å™¨
        kf = AdaptiveKalmanFilter(pair_name=pair, delta=delta, lambda_r=lambda_r)
        kf.warm_up_ols(x_data, y_data, 60)
        
        # è¿è¡Œæ»¤æ³¢è·å–ä¿¡å·
        z_scores = []
        innovations = []
        beta_values = []
        
        # è·³è¿‡90å¤©é¢„çƒ­æœŸ
        warmup_end = 90
        signal_period_dates = dates[warmup_end:]
        
        for i in range(warmup_end, len(x_data)):
            result = kf.update(y_data[i], x_data[i])
            z_scores.append(result['z'])
            innovations.append(result['v'])
            beta_values.append(result['beta'])
        
        if len(z_scores) < 100:
            return None
        
        # 1. Z>2ä¿¡å·æ¯”ä¾‹æ£€æŸ¥
        z_scores = np.array(z_scores)
        z_gt2_count = np.sum(np.abs(z_scores) > 2.0)
        z_gt2_ratio = z_gt2_count / len(z_scores)
        
        # 2. è®¡ç®—ä¿¡æ¯æ¯”ç‡(IR)
        # IR = æ”¶ç›Šå‡å€¼ / æ”¶ç›Šæ ‡å‡†å·®
        # è¿™é‡Œç”¨z_scoreçš„åè½¬ä½œä¸ºä¿¡å·ä»£ç†æ”¶ç›Š
        returns_proxy = -np.diff(z_scores)  # z_scoreä¸‹é™è¡¨ç¤ºæ”¶æ•›ï¼Œäº§ç”Ÿæ­£æ”¶ç›Š
        if len(returns_proxy) > 0:
            ir = np.mean(returns_proxy) / (np.std(returns_proxy) + 1e-8)
        else:
            ir = 0.0
        
        # 3. ä¸æ»šåŠ¨OLSçš„ç›¸å…³æ€§
        if len(x_data) >= 150:
            # è®¡ç®—60å¤©æ»šåŠ¨OLS beta
            rolling_betas = []
            for i in range(60, len(x_data)):
                x_window = x_data[i-60:i]
                y_window = y_data[i-60:i]
                reg = LinearRegression(fit_intercept=False)
                reg.fit(x_window.reshape(-1, 1), y_window)
                rolling_betas.append(reg.coef_[0])
            
            # å¯¹é½Kalman betaä¸æ»šåŠ¨OLS beta
            kalman_betas_aligned = beta_values[:len(rolling_betas)]
            if len(kalman_betas_aligned) > 30:
                ols_correlation, _ = pearsonr(kalman_betas_aligned, rolling_betas)
            else:
                ols_correlation = 0.0
        else:
            ols_correlation = 0.0
        
        # 4. å¹³ç¨³æ€§æ£€éªŒ
        try:
            adf_result = adfuller(innovations, autolag='AIC')
            adf_pvalue = adf_result[1]
            is_stationary = adf_pvalue < self.min_adf_pval
        except:
            adf_pvalue = 1.0
            is_stationary = False
        
        return {
            'pair': pair,
            'z_gt2_ratio': z_gt2_ratio,
            'ir': ir,
            'ols_correlation': ols_correlation,
            'adf_pvalue': adf_pvalue,
            'is_stationary': is_stationary,
            'innovation_std': np.std(innovations),
            'z_score_std': np.std(z_scores),
            'beta_stability': np.std(beta_values) / np.mean(beta_values) if np.mean(beta_values) != 0 else np.inf
        }
    
    def _calculate_composite_score(self, z_ratio, ir, ols_corr, adf_pval, stationary_ratio):
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        
        score = 0
        
        # Z>2æ¯”ä¾‹å¾—åˆ† (æƒé‡30%) - ç›®æ ‡2%-5%
        if self.target_z_ratio[0] <= z_ratio <= self.target_z_ratio[1]:
            z_score = 100  # åœ¨ç›®æ ‡èŒƒå›´å†…
        elif z_ratio < self.target_z_ratio[0]:
            z_score = max(0, 100 - (self.target_z_ratio[0] - z_ratio) * 2000)  # æƒ©ç½šè¿‡ä½
        else:
            z_score = max(0, 100 - (z_ratio - self.target_z_ratio[1]) * 1000)  # æƒ©ç½šè¿‡é«˜
        
        # IRå¾—åˆ† (æƒé‡25%) - è¶Šå¤§è¶Šå¥½
        ir_score = min(100, max(0, ir * 500 + 50))  # IRé€šå¸¸-1åˆ°1ä¹‹é—´
        
        # OLSç›¸å…³æ€§å¾—åˆ† (æƒé‡25%) - >0.6
        if ols_corr >= self.min_ols_corr:
            ols_score = 100
        else:
            ols_score = max(0, ols_corr / self.min_ols_corr * 100)
        
        # å¹³ç¨³æ€§å¾—åˆ† (æƒé‡20%) - å¹³ç¨³æ¯”ä¾‹è¶Šé«˜è¶Šå¥½
        stationarity_score = stationary_ratio * 100
        
        # ç»¼åˆå¾—åˆ†
        score = (z_score * 0.3 + ir_score * 0.25 + ols_score * 0.25 + stationarity_score * 0.2)
        
        return score
    
    def _analyze_optimization_results(self, results_df):
        """åˆ†æä¼˜åŒ–ç»“æœ"""
        
        print("\\nğŸ“Š å‚æ•°ä¼˜åŒ–ç»“æœåˆ†æ")
        print("=" * 60)
        
        # æŒ‰å¾—åˆ†æ’åº
        results_df = results_df.sort_values('score', ascending=False)
        
        print("\\nğŸ† æœ€ä¼˜å‚æ•°ç»„åˆ (Top 5):")
        top_results = results_df.head(5)
        
        for i, (_, row) in enumerate(top_results.iterrows()):
            print(f"\\n{i+1}. Î´={row['delta']:.2f}, Î»={row['lambda_r']:.2f} (å¾—åˆ†: {row['score']:.1f})")
            print(f"   Z>2æ¯”ä¾‹: {row['avg_z_ratio']*100:.1f}% (ç›®æ ‡: 2-5%)")
            print(f"   å¹³å‡IR: {row['avg_ir']:.3f}")
            print(f"   OLSç›¸å…³æ€§: {row['avg_ols_corr']:.3f} (ç›®æ ‡: >0.6)")
            print(f"   å¹³ç¨³æ¯”ä¾‹: {row['stationary_ratio']*100:.0f}%")
            print(f"   æœ‰æ•ˆé…å¯¹: {row['valid_pairs']}/5")
        
        # çº¦æŸæ¡ä»¶åˆ†æ
        print("\\nğŸ“‹ çº¦æŸæ¡ä»¶æ»¡è¶³æƒ…å†µ:")
        
        z_valid = results_df[
            (results_df['avg_z_ratio'] >= self.target_z_ratio[0]) & 
            (results_df['avg_z_ratio'] <= self.target_z_ratio[1])
        ]
        print(f"Z>2æ¯”ä¾‹åœ¨2-5%: {len(z_valid)}/{len(results_df)} ({len(z_valid)/len(results_df)*100:.1f}%)")
        
        ols_valid = results_df[results_df['avg_ols_corr'] >= self.min_ols_corr]
        print(f"OLSç›¸å…³æ€§>0.6: {len(ols_valid)}/{len(results_df)} ({len(ols_valid)/len(results_df)*100:.1f}%)")
        
        stationary_valid = results_df[results_df['stationary_ratio'] >= 0.6]
        print(f"å¹³ç¨³æ¯”ä¾‹>60%: {len(stationary_valid)}/{len(results_df)} ({len(stationary_valid)/len(results_df)*100:.1f}%)")
        
        # å…¨éƒ¨çº¦æŸåŒæ—¶æ»¡è¶³
        all_valid = results_df[
            (results_df['avg_z_ratio'] >= self.target_z_ratio[0]) & 
            (results_df['avg_z_ratio'] <= self.target_z_ratio[1]) &
            (results_df['avg_ols_corr'] >= self.min_ols_corr) &
            (results_df['stationary_ratio'] >= 0.6)
        ]
        print(f"åŒæ—¶æ»¡è¶³æ‰€æœ‰çº¦æŸ: {len(all_valid)}/{len(results_df)} ({len(all_valid)/len(results_df)*100:.1f}%)")
        
        if len(all_valid) > 0:
            print("\\nâœ… æ¨èå‚æ•° (æ»¡è¶³æ‰€æœ‰çº¦æŸçš„æœ€é«˜åˆ†):")
            best = all_valid.iloc[0]
            print(f"Î´ = {best['delta']:.2f}")
            print(f"Î» = {best['lambda_r']:.2f}")
            print(f"ç»¼åˆå¾—åˆ†: {best['score']:.1f}")
        else:
            print("\\nâš ï¸ æ²¡æœ‰å‚æ•°ç»„åˆåŒæ—¶æ»¡è¶³æ‰€æœ‰çº¦æŸï¼Œæ¨èç»¼åˆå¾—åˆ†æœ€é«˜çš„:")
            best = results_df.iloc[0]
            print(f"Î´ = {best['delta']:.2f}")  
            print(f"Î» = {best['lambda_r']:.2f}")
            print(f"ç»¼åˆå¾—åˆ†: {best['score']:.1f}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = f"kalman_optimization_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        results_df.to_csv(output_file, index=False)
        print(f"\\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")
        
        return results_df

def main():
    """ä¸»å‡½æ•°"""
    optimizer = KalmanParameterOptimizer()
    results = optimizer.optimize_parameters()
    
    if results is not None:
        print("\\nğŸ¯ å‚æ•°ä¼˜åŒ–å®Œæˆï¼è¯·æ ¹æ®æ¨èå‚æ•°æ›´æ–° lib/signal_generation.py")
    else:
        print("\\nâŒ å‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…å¯¹è®¾ç½®")

if __name__ == "__main__":
    main()